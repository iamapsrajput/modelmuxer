# ModelMuxer (c) 2025 Ajay Rajput
# Licensed under Business Source License 1.1 â€“ see LICENSE for details.
"""
Redis-based caching implementation for ModelMuxer.

This module provides Redis-based caching with advanced features like
TTL management, compression, and intelligent cache warming.
"""

from typing import Any

import redis.asyncio as redis
import structlog
from redis.asyncio import Redis

from ..core.exceptions import CacheError
from ..core.interfaces import CacheInterface
from ..core.serialization import secure_serializer

# Removed unused import: hash_prompt

logger = structlog.get_logger(__name__)


class RedisCache(CacheInterface):
    """
    Redis-based cache implementation with advanced features.

    Provides high-performance caching with compression, TTL management,
    and intelligent cache warming strategies.
    """

    def __init__(
        self,
        redis_url: str = "redis://localhost:6379",
        db: int = 0,
        key_prefix: str = "modelmuxer:",
        default_ttl: int = 3600,
        compression_enabled: bool = True,
        compression_threshold: int = 1024,
        max_connections: int = 10,
    ):
        self.redis_url = redis_url
        self.db = db
        self.key_prefix = key_prefix
        self.default_ttl = default_ttl
        self.compression_enabled = compression_enabled
        self.compression_threshold = compression_threshold

        # Redis connection
        self.redis: Redis | None = None
        self.connection_pool: redis.ConnectionPool | None = None

        # Cache statistics
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "errors": 0,
            "compression_saves": 0,
        }

        # Initialize connection
        self._initialize_connection(max_connections)

        logger.info(
            "redis_cache_initialized", redis_url=redis_url, db=db, compression=compression_enabled
        )

    def _initialize_connection(self, max_connections: int) -> None:
        """Initialize Redis connection pool."""
        try:
            self.connection_pool = redis.ConnectionPool.from_url(
                self.redis_url,
                db=self.db,
                max_connections=max_connections,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={},
            )

            self.redis = Redis(connection_pool=self.connection_pool)

        except Exception as e:
            logger.error("redis_connection_failed", error=str(e))
            raise CacheError(f"Failed to initialize Redis connection: {e}") from e

    def _make_key(self, key: str) -> str:
        """Create a prefixed cache key."""
        return f"{self.key_prefix}{key}"

    def _serialize_value(self, value: Any) -> bytes:
        """Serialize and optionally compress a value using secure JSON serialization."""
        try:
            # Use secure serializer instead of pickle
            serialized_data = secure_serializer.serialize(value)

            # Track compression if it was used
            if serialized_data.startswith(b"compressed:"):
                self.stats["compression_saves"] += 1

            return serialized_data

        except Exception as e:
            raise CacheError(f"Failed to serialize value: {e}") from e

    def _deserialize_value(self, data: bytes) -> Any:
        """Deserialize and optionally decompress a value using secure JSON deserialization."""
        try:
            # Try secure deserialization first
            try:
                return secure_serializer.deserialize(data)
            except ValueError:
                # Fallback to legacy pickle format for backward compatibility
                logger.warning(
                    "falling_back_to_pickle_deserialization", reason="secure_deserialization_failed"
                )

                if data.startswith(b"compressed:"):
                    # Legacy compressed pickle format
                    import gzip
                    import pickle  # noqa: S403 - controlled legacy fallback

                    compressed_data = data[11:]  # Remove "compressed:" prefix
                    decompressed = gzip.decompress(compressed_data)
                    return pickle.loads(decompressed)  # noqa: S301 - controlled legacy fallback
                elif data.startswith(b"raw:"):
                    # Legacy raw pickle format
                    import pickle  # noqa: S403 - controlled legacy fallback

                    raw_data = data[4:]  # Remove "raw:" prefix
                    return pickle.loads(raw_data)  # noqa: S301 - controlled legacy fallback
                else:
                    # Legacy format - assume raw pickle
                    import pickle  # noqa: S403 - controlled legacy fallback

                    return pickle.loads(data)  # noqa: S301 - controlled legacy fallback

        except Exception as e:
            raise CacheError(f"Failed to deserialize value: {e}") from e

    async def get(self, key: str) -> Any | None:
        """Get a value from cache."""
        if not self.redis:
            return None

        try:
            cache_key = self._make_key(key)
            data = await self.redis.get(cache_key)

            if data is None:
                self.stats["misses"] += 1
                return None

            value = self._deserialize_value(data)
            self.stats["hits"] += 1

            logger.debug("cache_hit", key=key)
            return value

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_get_error", key=key, error=str(e))
            return None

    async def set(self, key: str, value: Any, ttl: int | None = None) -> bool:
        """Set a value in cache with optional TTL."""
        if not self.redis:
            return False

        try:
            cache_key = self._make_key(key)
            serialized_value = self._serialize_value(value)

            # Use provided TTL or default
            expiry = ttl if ttl is not None else self.default_ttl

            if expiry > 0:
                await self.redis.setex(cache_key, expiry, serialized_value)
            else:
                await self.redis.set(cache_key, serialized_value)

            self.stats["sets"] += 1
            logger.debug("cache_set", key=key, ttl=expiry)
            return True

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_set_error", key=key, error=str(e))
            return False

    async def delete(self, key: str) -> bool:
        """Delete a value from cache."""
        if not self.redis:
            return False

        try:
            cache_key = self._make_key(key)
            result = await self.redis.delete(cache_key)

            self.stats["deletes"] += 1
            logger.debug("cache_delete", key=key, existed=bool(result))
            return bool(result)

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_delete_error", key=key, error=str(e))
            return False

    async def exists(self, key: str) -> bool:
        """Check if a key exists in cache."""
        if not self.redis:
            return False

        try:
            cache_key = self._make_key(key)
            result = await self.redis.exists(cache_key)
            return bool(result)

        except Exception as e:
            logger.error("cache_exists_error", key=key, error=str(e))
            return False

    async def get_ttl(self, key: str) -> int | None:
        """Get the TTL of a key."""
        if not self.redis:
            return None

        try:
            cache_key = self._make_key(key)
            ttl = await self.redis.ttl(cache_key)

            if ttl == -2:  # Key doesn't exist
                return None
            elif ttl == -1:  # Key exists but has no expiry
                return -1
            else:
                return int(ttl)

        except Exception as e:
            logger.error("cache_ttl_error", key=key, error=str(e))
            return None

    async def extend_ttl(self, key: str, additional_seconds: int) -> bool:
        """Extend the TTL of a key."""
        if not self.redis:
            return False

        try:
            cache_key = self._make_key(key)
            current_ttl = await self.redis.ttl(cache_key)

            if current_ttl > 0:
                new_ttl = current_ttl + additional_seconds
                await self.redis.expire(cache_key, new_ttl)
                return True

            return False

        except Exception as e:
            logger.error("cache_extend_ttl_error", key=key, error=str(e))
            return False

    async def get_multiple(self, keys: list[str]) -> dict[str, Any]:
        """Get multiple values from cache."""
        if not self.redis or not keys:
            return {}

        try:
            cache_keys = [self._make_key(key) for key in keys]
            values = await self.redis.mget(cache_keys)

            result = {}
            for _i, (original_key, data) in enumerate(zip(keys, values, strict=False)):
                if data is not None:
                    try:
                        result[original_key] = self._deserialize_value(data)
                        self.stats["hits"] += 1
                    except Exception as e:
                        logger.warning("cache_deserialize_error", key=original_key, error=str(e))
                        self.stats["errors"] += 1
                else:
                    self.stats["misses"] += 1

            return result

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_get_multiple_error", keys=keys, error=str(e))
            return {}

    async def set_multiple(self, items: dict[str, Any], ttl: int | None = None) -> bool:
        """Set multiple values in cache."""
        if not self.redis or not items:
            return True

        try:
            # Prepare data for mset
            cache_data = {}
            for key, value in items.items():
                cache_key = self._make_key(key)
                serialized_value = self._serialize_value(value)
                cache_data[cache_key] = serialized_value

            # Set all values
            await self.redis.mset(cache_data)

            # Set TTL if specified
            expiry = ttl if ttl is not None else self.default_ttl
            if expiry > 0:
                for cache_key in cache_data.keys():
                    await self.redis.expire(cache_key, expiry)

            self.stats["sets"] += len(items)
            return True

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_set_multiple_error", items_count=len(items), error=str(e))
            return False

    async def clear_pattern(self, pattern: str) -> int:
        """Clear all keys matching a pattern."""
        if not self.redis:
            return 0

        try:
            cache_pattern = self._make_key(pattern)
            keys = []

            # Scan for matching keys
            async for key in self.redis.scan_iter(match=cache_pattern):
                keys.append(key)

            if keys:
                deleted_count = await self.redis.delete(*keys)
                self.stats["deletes"] += deleted_count
                logger.info("cache_pattern_cleared", pattern=pattern, deleted=deleted_count)
                return int(deleted_count)

            return 0

        except Exception as e:
            self.stats["errors"] += 1
            logger.error("cache_clear_pattern_error", pattern=pattern, error=str(e))
            return 0

    async def get_cache_info(self) -> dict[str, Any]:
        """Get cache information and statistics."""
        if not self.redis:
            return {"status": "disconnected"}

        try:
            info = await self.redis.info()

            # Calculate hit rate
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = self.stats["hits"] / max(total_requests, 1)

            return {
                "status": "connected",
                "redis_info": {
                    "version": info.get("redis_version"),
                    "used_memory": info.get("used_memory"),
                    "used_memory_human": info.get("used_memory_human"),
                    "connected_clients": info.get("connected_clients"),
                    "total_commands_processed": info.get("total_commands_processed"),
                },
                "cache_stats": self.stats.copy(),
                "hit_rate": hit_rate,
                "compression_enabled": self.compression_enabled,
            }

        except Exception as e:
            logger.error("cache_info_error", error=str(e))
            return {"status": "error", "error": str(e)}

    async def health_check(self) -> bool:
        """Check if Redis is healthy."""
        if not self.redis:
            return False

        try:
            # Try a simple ping
            pong = await self.redis.ping()
            return bool(pong == b"PONG" or pong == "PONG")

        except Exception as e:
            logger.error("redis_health_check_failed", error=str(e))
            return False

    async def clear(self) -> bool:
        """Clear all cache entries."""
        if not self.redis:
            return False

        try:
            # Clear all keys with our prefix
            pattern = self._make_key("*")
            keys = []

            async for key in self.redis.scan_iter(match=pattern):
                keys.append(key)

            if keys:
                deleted_count = await self.redis.delete(*keys)
                self.stats["deletes"] += deleted_count
                logger.info("redis_cache_cleared", deleted_count=deleted_count)
                return True

            return True

        except Exception as e:
            logger.error("redis_cache_clear_failed", error=str(e))
            return False

    async def close(self) -> None:
        """Close Redis connection."""
        if self.redis:
            await self.redis.close()
            logger.info("redis_connection_closed")

    async def get_stats(self) -> dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / max(total_requests, 1)

        return {**self.stats, "total_requests": total_requests, "hit_rate": hit_rate}
