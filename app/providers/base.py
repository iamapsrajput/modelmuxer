# ModelMuxer (c) 2025 Ajay Rajput
# Licensed under Business Source License 1.1 â€“ see LICENSE for details.
"""
Abstract base class for LLM providers.

This module contains legacy provider interfaces (chat_completion, streaming)
and the new unified Provider Adapter interface (invoke) with shared
resilience helpers and a common ProviderResponse dataclass.
"""

import uuid
from abc import ABC, abstractmethod
from collections.abc import AsyncGenerator
from datetime import datetime
from typing import Any, Optional

import httpx

from ..models import ChatCompletionResponse, ChatMessage, Choice, RouterMetadata, Usage
from ..security.config import SecurityConfig


class ProviderError(Exception):
    """Base exception for provider errors."""

    def __init__(self, message: str, status_code: int | None = None, provider: str | None = None):
        self.message = message
        self.status_code = status_code
        self.provider = provider
        super().__init__(message)


class RateLimitError(ProviderError):
    """Exception for rate limit errors."""

    pass


class AuthenticationError(ProviderError):
    """Exception for authentication errors."""

    pass


class ModelNotFoundError(ProviderError):
    """Exception for model not found errors."""

    pass


class LLMProvider(ABC):
    """Legacy abstract base class for providers using chat_completion interface."""

    def __init__(self, api_key: str, base_url: str, provider_name: str):
        self.api_key = api_key
        self.base_url = base_url
        self.provider_name = provider_name
        # Use secure HTTP client configuration
        self.client = SecurityConfig.get_secure_httpx_client()

    async def __aenter__(self) -> "LLMProvider":
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        await self.client.aclose()

    @abstractmethod
    async def chat_completion(
        self,
        messages: list[ChatMessage],
        model: str,
        max_tokens: int | None = None,
        temperature: float | None = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> ChatCompletionResponse:
        """
        Generate a chat completion.

        Args:
            messages: List of chat messages
            model: Model name to use
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            stream: Whether to stream the response
            **kwargs: Additional provider-specific parameters

        Returns:
            ChatCompletionResponse with standardized format

        Raises:
            ProviderError: For various provider-specific errors
        """
        raise NotImplementedError

    @abstractmethod
    async def stream_chat_completion(
        self,
        messages: list[ChatMessage],
        model: str,
        max_tokens: int | None = None,
        temperature: float | None = None,
        **kwargs: Any,
    ) -> AsyncGenerator[dict[str, Any], None]:
        """
        Stream a chat completion.

        Args:
            messages: List of chat messages
            model: Model name to use
            max_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            **kwargs: Additional provider-specific parameters

        Yields:
            Streaming response chunks in OpenAI format
        """
        raise NotImplementedError

    @abstractmethod
    def calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """
        Calculate the cost for a request.

        Args:
            input_tokens: Number of input tokens
            output_tokens: Number of output tokens
            model: Model name used

        Returns:
            Cost in USD
        """
        raise NotImplementedError

    @abstractmethod
    def get_supported_models(self) -> list[str]:
        """
        Get list of supported models for this provider.

        Returns:
            List of model names
        """
        raise NotImplementedError

    def _create_headers(self) -> dict[str, str]:
        """Create headers for API requests."""
        return {
            "Content-Type": "application/json",
            "User-Agent": f"ModelMuxer/1.0.0 ({self.provider_name})",
        }

    def _handle_http_error(self, response: httpx.Response) -> None:
        """Handle HTTP errors and convert to appropriate exceptions."""
        if response.status_code == 401:
            raise AuthenticationError(
                f"Authentication failed for {self.provider_name}",
                status_code=401,
                provider=self.provider_name,
            )
        elif response.status_code == 404:
            raise ModelNotFoundError(
                f"Model not found on {self.provider_name}",
                status_code=404,
                provider=self.provider_name,
            )
        elif response.status_code == 429:
            raise RateLimitError(
                f"Rate limit exceeded for {self.provider_name}",
                status_code=429,
                provider=self.provider_name,
            )
        elif response.status_code >= 400:
            try:
                error_data = response.json()
                error_message = error_data.get("error", {}).get("message", "Unknown error")
            except (ValueError, KeyError, TypeError):
                error_message = f"HTTP {response.status_code} error"

            raise ProviderError(
                f"{self.provider_name} error: {error_message}",
                status_code=response.status_code,
                provider=self.provider_name,
            )

    def _create_standard_response(
        self,
        content: str,
        model: str,
        input_tokens: int,
        output_tokens: int,
        routing_reason: str,
        response_time_ms: float,
        finish_reason: str = "stop",
    ) -> ChatCompletionResponse:
        """Create a standardized response format."""
        cost = self.calculate_cost(input_tokens, output_tokens, model)

        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:29]}",
            object="chat.completion",
            created=int(datetime.now().timestamp()),
            model=model,
            choices=[
                Choice(
                    index=0,
                    message=ChatMessage(role="assistant", content=content, name=None),
                    finish_reason=finish_reason,
                )
            ],
            usage=Usage(
                prompt_tokens=input_tokens,
                completion_tokens=output_tokens,
                total_tokens=input_tokens + output_tokens,
            ),
            router_metadata=RouterMetadata(
                selected_provider=self.provider_name,
                selected_model=model,
                routing_reason=routing_reason,
                estimated_cost=cost,
                response_time_ms=response_time_ms,
            ),
        )

    async def health_check(self) -> bool:
        """Check if the provider is healthy and accessible."""
        try:
            # Simple health check - attempt to make a minimal request
            test_messages = [ChatMessage(role="user", content="Hi", name=None)]
            models = self.get_supported_models()
            if not models:
                return False

            # Use the first available model for health check
            await self.chat_completion(
                messages=test_messages, model=models[0], max_tokens=1, temperature=0.0
            )
            return True
        except Exception:
            return False


# ===================== New Adapter Interface =====================
from dataclasses import dataclass
import time

from app.settings import settings


@dataclass
class ProviderResponse:
    """Standard adapter response used across providers.

    - output_text: final assistant text
    - tokens_in/tokens_out: token counts from provider
    - latency_ms: total latency in milliseconds
    - raw: raw provider payload (optional)
    - error: error string if any
    """

    output_text: str
    tokens_in: int
    tokens_out: int
    latency_ms: int
    raw: Optional[Any] = None
    error: Optional[str] = None


class SimpleCircuitBreaker:
    """Minimal circuit breaker for adapters."""

    def __init__(self) -> None:
        self.failures = 0
        self.open_until: float = 0.0

    def is_open(self) -> bool:
        return time.time() < self.open_until

    def on_failure(self) -> None:
        self.failures += 1
        if self.failures >= settings.providers.circuit_fail_threshold:
            self.open_until = time.time() + settings.providers.circuit_cooldown_sec

    def on_success(self) -> None:
        self.failures = 0
        self.open_until = 0.0


class LLMProviderAdapter(ABC):
    """Abstract provider adapter with a unified invoke() method."""

    @abstractmethod
    async def invoke(self, model: str, prompt: str, **kwargs: Any) -> ProviderResponse:
        """Invoke the provider synchronously and return standardized response."""
        raise NotImplementedError
