# ModelMuxer (c) 2025 Ajay Rajput
# Licensed under Business Source License 1.1 â€“ see LICENSE for details.
"""
Google Gemini provider implementation.

This module provides integration with Google's Gemini API for the ModelMuxer
LLM routing system.
"""

import json
import time
from collections.abc import AsyncGenerator
from typing import Any

import httpx
import structlog

from ..core.utils import estimate_tokens
from ..models import ChatCompletionResponse, ChatMessage
from .base import AuthenticationError, LLMProvider, ProviderError, RateLimitError

logger = structlog.get_logger(__name__)


class GoogleProvider(LLMProvider):
    """Google Gemini API provider implementation."""

    def __init__(self, api_key: str | None = None):
        if not api_key:
            raise AuthenticationError("Google API key is required")

        super().__init__(
            api_key=api_key,
            base_url="https://generativelanguage.googleapis.com/v1beta",
            provider_name="google",
        )

        # Pricing per million tokens (as of 2024)
        self.pricing = {
            "gemini-1.5-pro": {"input": 3.5, "output": 10.5},
            "gemini-1.5-flash": {"input": 0.075, "output": 0.3},
            "gemini-1.0-pro": {"input": 0.5, "output": 1.5},
        }

        self.supported_models = list(self.pricing.keys())

        # Rate limits (requests per minute)
        self.rate_limits = {"gemini-1.5-pro": 60, "gemini-1.5-flash": 1000, "gemini-1.0-pro": 60}

    def _create_headers(self) -> dict[str, str]:
        """Create headers for Google API requests."""
        return {
            "Content-Type": "application/json",
            "User-Agent": "ModelMuxer/1.0.0 (Google Gemini)",
        }

    def get_supported_models(self) -> list[str]:
        """Get list of supported Google models."""
        return self.supported_models

    def calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """Calculate cost for Google request."""
        if model not in self.pricing:
            model = "gemini-1.5-flash"  # Default fallback

        model_pricing = self.pricing[model]
        input_cost = (input_tokens / 1_000_000) * model_pricing["input"]
        output_cost = (output_tokens / 1_000_000) * model_pricing["output"]

        return input_cost + output_cost

    def get_rate_limits(self) -> dict[str, Any]:
        """Get rate limit information."""
        return {
            "requests_per_minute": self.rate_limits,
            "tokens_per_minute": {
                "gemini-1.5-pro": 32000,
                "gemini-1.5-flash": 1000000,
                "gemini-1.0-pro": 32000,
            },
        }

    def _convert_messages_to_google_format(self, messages: list[ChatMessage]) -> dict[str, Any]:
        """Convert OpenAI format messages to Google Gemini format."""
        contents = []
        system_instruction = None

        for msg in messages:
            if msg.role == "system":
                # Google uses systemInstruction separately
                system_instruction = msg.content
            elif msg.role == "user":
                contents.append({"role": "user", "parts": [{"text": msg.content}]})
            elif msg.role == "assistant":
                contents.append(
                    {
                        "role": "model",  # Google uses "model" instead of "assistant"
                        "parts": [{"text": msg.content}],
                    }
                )

        result = {"contents": contents}
        if system_instruction:
            result["systemInstruction"] = {"parts": [{"text": system_instruction}]}

        return result

    async def chat_completion(
        self,
        messages: list[ChatMessage],
        model: str = "gemini-1.5-flash",
        max_tokens: int | None = None,
        temperature: float | None = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> ChatCompletionResponse:
        """Generate a chat completion using Google Gemini API."""
        start_time = time.time()

        # Convert messages to Google format
        google_messages = self._convert_messages_to_google_format(messages)

        # Prepare request payload
        payload = google_messages.copy()

        # Add generation config
        generation_config = {}
        if temperature is not None:
            generation_config["temperature"] = temperature
        if max_tokens is not None:
            generation_config["maxOutputTokens"] = max_tokens

        if generation_config:
            payload["generationConfig"] = generation_config

        # Add safety settings (optional)
        payload["safetySettings"] = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]

        try:
            url = f"{self.base_url}/models/{model}:generateContent"

            response = await self.client.post(
                url,
                params={"key": self.api_key},
                json=payload,
                headers=self._create_headers(),
                timeout=60.0,
            )

            self._handle_http_error(response)
            response_data = response.json()

            # Calculate response time
            response_time_ms = (time.time() - start_time) * 1000

            # Extract content from Google response
            content = ""
            finish_reason = "stop"

            if "candidates" in response_data and response_data["candidates"]:
                candidate = response_data["candidates"][0]

                if "content" in candidate and "parts" in candidate["content"]:
                    parts = candidate["content"]["parts"]
                    content = "".join([part.get("text", "") for part in parts])

                # Map Google finish reasons to OpenAI format
                google_finish_reason = candidate.get("finishReason", "STOP")
                finish_reason_mapping = {
                    "STOP": "stop",
                    "MAX_TOKENS": "length",
                    "SAFETY": "content_filter",
                    "RECITATION": "content_filter",
                    "OTHER": "stop",
                }
                finish_reason = finish_reason_mapping.get(google_finish_reason, "stop")

            # Estimate token usage (Google doesn't always provide exact counts)
            input_tokens = sum(estimate_tokens(msg.content, model) for msg in messages)
            output_tokens = estimate_tokens(content, model)

            # Check if Google provided usage metadata
            if "usageMetadata" in response_data:
                usage_metadata = response_data["usageMetadata"]
                input_tokens = usage_metadata.get("promptTokenCount", input_tokens)
                output_tokens = usage_metadata.get("candidatesTokenCount", output_tokens)

            # Create standardized response
            return self._create_standard_response(
                content=content,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                routing_reason="Selected Google Gemini for request",
                response_time_ms=response_time_ms,
                finish_reason=finish_reason,
            )

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                raise RateLimitError(
                    "Google API rate limit exceeded", provider=self.provider_name
                ) from e
            elif e.response.status_code == 401:
                raise AuthenticationError(
                    "Google API authentication failed", provider=self.provider_name
                ) from e
            else:
                raise ProviderError(
                    f"Google API error: {e.response.status_code}",
                    provider=self.provider_name,
                    status_code=e.response.status_code,
                ) from e
        except httpx.RequestError as e:
            raise ProviderError(
                f"Google request failed: {str(e)}", provider=self.provider_name
            ) from e
        except Exception as e:
            if isinstance(e, ProviderError):
                raise
            raise ProviderError(
                f"Google unexpected error: {str(e)}", provider=self.provider_name
            ) from e

    async def stream_chat_completion(
        self,
        messages: list[ChatMessage],
        model: str,
        max_tokens: int | None = None,
        temperature: float | None = None,
        **kwargs: Any,
    ) -> AsyncGenerator[dict[str, Any], None]:
        """Stream a chat completion using Google Gemini API."""
        # Convert messages to Google format
        google_messages = self._convert_messages_to_google_format(messages)

        # Prepare request payload
        payload = google_messages.copy()

        # Add generation config
        generation_config = {}
        if temperature is not None:
            generation_config["temperature"] = temperature
        if max_tokens is not None:
            generation_config["maxOutputTokens"] = max_tokens

        if generation_config:
            payload["generationConfig"] = generation_config

        try:
            url = f"{self.base_url}/models/{model}:streamGenerateContent"

            async with self.client.stream(
                "POST",
                url,
                params={"key": self.api_key},
                json=payload,
                headers=self._create_headers(),
                timeout=120.0,
            ) as response:
                self._handle_http_error(response)

                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            # Google returns JSON objects separated by newlines
                            chunk_data = json.loads(line)

                            # Convert to OpenAI streaming format
                            if "candidates" in chunk_data and chunk_data["candidates"]:
                                candidate = chunk_data["candidates"][0]

                                if "content" in candidate and "parts" in candidate["content"]:
                                    parts = candidate["content"]["parts"]
                                    text = "".join([part.get("text", "") for part in parts])

                                    if text:
                                        openai_chunk = {
                                            "id": f"chatcmpl-google-{int(time.time())}",
                                            "object": "chat.completion.chunk",
                                            "created": int(time.time()),
                                            "model": model,
                                            "choices": [
                                                {
                                                    "index": 0,
                                                    "delta": {"content": text},
                                                    "finish_reason": None,
                                                }
                                            ],
                                        }
                                        yield openai_chunk

                                # Check for finish reason
                                if "finishReason" in candidate:
                                    final_chunk = {
                                        "id": f"chatcmpl-google-{int(time.time())}",
                                        "object": "chat.completion.chunk",
                                        "created": int(time.time()),
                                        "model": model,
                                        "choices": [
                                            {"index": 0, "delta": {}, "finish_reason": "stop"}
                                        ],
                                    }
                                    yield final_chunk
                                    break

                        except json.JSONDecodeError:
                            continue

        except httpx.RequestError as e:
            raise ProviderError(
                f"Google streaming request failed: {str(e)}", provider=self.provider_name
            ) from e
        except Exception as e:
            if isinstance(e, ProviderError):
                raise
            raise ProviderError(
                f"Google streaming unexpected error: {str(e)}", provider=self.provider_name
            ) from e

    async def health_check(self) -> bool:
        """Check if Google Gemini API is accessible."""
        try:
            test_messages = [ChatMessage(role="user", content="Hi", name=None)]
            await self.chat_completion(
                messages=test_messages, model="gemini-1.5-flash", max_tokens=1
            )
            return True
        except Exception as e:
            logger.warning("google_health_check_failed", error=str(e))
            return False
