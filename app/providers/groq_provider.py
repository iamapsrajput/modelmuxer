# ModelMuxer (c) 2025 Ajay Rajput
# Licensed under Business Source License 1.1 â€“ see LICENSE for details.
"""
Groq provider implementation.

This module provides integration with Groq's API for the ModelMuxer
LLM routing system. Groq provides fast inference for open-source models.
"""

import json
import time
from collections.abc import AsyncGenerator
from typing import Any

import httpx
import structlog

from ..models import ChatCompletionResponse, ChatMessage
from .base import AuthenticationError, LLMProvider, ProviderError, RateLimitError

logger = structlog.get_logger(__name__)


class GroqProvider(LLMProvider):
    """Groq API provider implementation."""

    def __init__(self, api_key: str | None = None):
        if not api_key:
            raise AuthenticationError("Groq API key is required")

        super().__init__(
            api_key=api_key, base_url="https://api.groq.com/openai/v1", provider_name="groq"
        )

        # Pricing per million tokens (as of 2024) - Groq is very cost-effective
        self.pricing = {
            "llama-3.1-70b-versatile": {"input": 0.59, "output": 0.79},
            "llama-3.1-8b-instant": {"input": 0.05, "output": 0.08},
            "mixtral-8x7b-32768": {"input": 0.24, "output": 0.24},
            "gemma-7b-it": {"input": 0.07, "output": 0.07},
            "gemma2-9b-it": {"input": 0.20, "output": 0.20},
        }

        self.supported_models = list(self.pricing.keys())

        # Rate limits (requests per minute) - Groq has generous limits
        self.rate_limits = {
            "llama-3.1-70b-versatile": 30,
            "llama-3.1-8b-instant": 30,
            "mixtral-8x7b-32768": 30,
            "gemma-7b-it": 30,
            "gemma2-9b-it": 30,
        }

    def _create_headers(self) -> dict[str, str]:
        """Create headers for Groq API requests."""
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "User-Agent": "ModelMuxer/1.0.0 (Groq)",
        }

    def get_supported_models(self) -> list[str]:
        """Get list of supported Groq models."""
        return self.supported_models

    def calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -> float:
        """Calculate cost for Groq request."""
        if model not in self.pricing:
            model = "llama-3.1-8b-instant"  # Default fallback

        model_pricing = self.pricing[model]
        input_cost = (input_tokens / 1_000_000) * model_pricing["input"]
        output_cost = (output_tokens / 1_000_000) * model_pricing["output"]

        return input_cost + output_cost

    def get_rate_limits(self) -> dict[str, Any]:
        """Get rate limit information."""
        return {
            "requests_per_minute": self.rate_limits,
            "tokens_per_minute": {
                "llama-3.1-70b-versatile": 6000,
                "llama-3.1-8b-instant": 30000,
                "mixtral-8x7b-32768": 5000,
                "gemma-7b-it": 15000,
                "gemma2-9b-it": 15000,
            },
        }

    def _prepare_messages(self, messages: list[ChatMessage]) -> list[dict[str, str]]:
        """Convert ChatMessage objects to Groq format (OpenAI-compatible)."""
        return [
            {"role": msg.role, "content": msg.content, **({"name": msg.name} if msg.name else {})}
            for msg in messages
        ]

    async def chat_completion(
        self,
        messages: list[ChatMessage],
        model: str = "llama-3.1-8b-instant",
        max_tokens: int | None = None,
        temperature: float | None = None,
        stream: bool = False,
        **kwargs: Any,
    ) -> ChatCompletionResponse:
        """Generate a chat completion using Groq API."""
        start_time = time.time()

        # Prepare request payload (OpenAI-compatible)
        payload = {"model": model, "messages": self._prepare_messages(messages), "stream": stream}

        # Add optional parameters
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        if temperature is not None:
            payload["temperature"] = temperature

        # Add any additional kwargs
        for key, value in kwargs.items():
            if key not in ["messages", "model", "stream"] and value is not None:
                payload[key] = value

        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=self._create_headers(),
                timeout=60.0,
            )

            self._handle_http_error(response)
            response_data = response.json()

            # Calculate response time
            response_time_ms = (time.time() - start_time) * 1000

            # Extract usage information
            usage = response_data.get("usage", {})
            input_tokens = usage.get("prompt_tokens", 0)
            output_tokens = usage.get("completion_tokens", 0)

            # Extract content
            choices = response_data.get("choices", [])
            if not choices:
                raise ProviderError("No choices returned from Groq", provider=self.provider_name)

            content = choices[0].get("message", {}).get("content", "")
            finish_reason = choices[0].get("finish_reason", "stop")

            # Create standardized response
            return self._create_standard_response(
                content=content,
                model=model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                routing_reason="Selected Groq for fast inference",
                response_time_ms=response_time_ms,
                finish_reason=finish_reason,
            )

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 429:
                raise RateLimitError(
                    "Groq API rate limit exceeded", provider=self.provider_name
                ) from e
            elif e.response.status_code == 401:
                raise AuthenticationError(
                    "Groq API authentication failed", provider=self.provider_name
                ) from e
            else:
                error_detail = ""
                try:
                    error_data = e.response.json()
                    error_detail = error_data.get("error", {}).get("message", "")
                except (ValueError, KeyError, TypeError, AttributeError):
                    pass

                raise ProviderError(
                    f"Groq API error: {e.response.status_code} {error_detail}",
                    provider=self.provider_name,
                    status_code=e.response.status_code,
                ) from e
        except httpx.RequestError as e:
            raise ProviderError(
                f"Groq request failed: {str(e)}", provider=self.provider_name
            ) from e
        except Exception as e:
            if isinstance(e, ProviderError):
                raise
            raise ProviderError(
                f"Groq unexpected error: {str(e)}", provider=self.provider_name
            ) from e

    async def stream_chat_completion(
        self,
        messages: list[ChatMessage],
        model: str,
        max_tokens: int | None = None,
        temperature: float | None = None,
        **kwargs: Any,
    ) -> AsyncGenerator[dict[str, Any], None]:
        """Stream a chat completion using Groq API."""
        # Prepare request payload
        payload = {"model": model, "messages": self._prepare_messages(messages), "stream": True}

        # Add optional parameters
        if max_tokens is not None:
            payload["max_tokens"] = max_tokens
        if temperature is not None:
            payload["temperature"] = temperature

        # Add any additional kwargs
        for key, value in kwargs.items():
            if key not in ["messages", "model", "stream"] and value is not None:
                payload[key] = value

        try:
            async with self.client.stream(
                "POST",
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=self._create_headers(),
                timeout=120.0,
            ) as response:
                self._handle_http_error(response)

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data = line[6:]  # Remove "data: " prefix

                        if data == "[DONE]":
                            break

                        try:
                            chunk = json.loads(data)
                            yield chunk
                        except json.JSONDecodeError:
                            continue

        except httpx.RequestError as e:
            raise ProviderError(
                f"Groq streaming request failed: {str(e)}", provider=self.provider_name
            ) from e
        except Exception as e:
            if isinstance(e, ProviderError):
                raise
            raise ProviderError(
                f"Groq streaming unexpected error: {str(e)}", provider=self.provider_name
            ) from e

    async def health_check(self) -> bool:
        """Check if Groq API is accessible."""
        try:
            test_messages = [ChatMessage(role="user", content="Hi", name=None)]
            await self.chat_completion(
                messages=test_messages, model="llama-3.1-8b-instant", max_tokens=1
            )
            return True
        except Exception as e:
            logger.warning("groq_health_check_failed", error=str(e))
            return False

    def get_model_info(self, model: str) -> dict[str, Any]:
        """Get detailed information about a Groq model."""
        model_info = {
            "llama-3.1-70b-versatile": {
                "description": "Meta's Llama 3.1 70B model, versatile for many tasks",
                "context_length": 131072,
                "strengths": ["reasoning", "code", "math"],
                "speed": "medium",
            },
            "llama-3.1-8b-instant": {
                "description": "Meta's Llama 3.1 8B model, optimized for speed",
                "context_length": 131072,
                "strengths": ["speed", "general tasks"],
                "speed": "very fast",
            },
            "mixtral-8x7b-32768": {
                "description": "Mistral's Mixtral 8x7B mixture of experts model",
                "context_length": 32768,
                "strengths": ["multilingual", "code", "reasoning"],
                "speed": "fast",
            },
            "gemma-7b-it": {
                "description": "Google's Gemma 7B instruction-tuned model",
                "context_length": 8192,
                "strengths": ["instruction following", "safety"],
                "speed": "fast",
            },
            "gemma2-9b-it": {
                "description": "Google's Gemma 2 9B instruction-tuned model",
                "context_length": 8192,
                "strengths": ["improved performance", "efficiency"],
                "speed": "fast",
            },
        }

        return model_info.get(
            model,
            {
                "description": "Unknown model",
                "context_length": 4096,
                "strengths": [],
                "speed": "unknown",
            },
        )
